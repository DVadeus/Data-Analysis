{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice cleaning missing values with California Cities Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will practice how to identifying and cleaning missing (null) values. For this project, we define \"missing\" only as np.nan values. First, we will practice how to find the missing values, drop columns or rows having missing values, and then we will practice different methods of filling the missing values. We will cover other missing values (for example, empty strings, etc) in other projects.\n",
    "\n",
    "**Our main focus will be on the pandas methods for:**\n",
    "\n",
    "1. Identification and Cleaning Missing Values:\n",
    "\n",
    "    - ``.info()``\n",
    "    - ``.isna()``\n",
    "    - ``.isnull()``\n",
    "    - ``.dropna()``\n",
    "\n",
    "2. Data Imputation Methods:\n",
    "\n",
    "    - ``.fillna()``\n",
    "    - ``.bfill()``\n",
    "    - ``.ffill()``\n",
    "    - ``.mode()``\n",
    "    - ``.mean()``\n",
    "    - ``.median()``\n",
    "    \n",
    "Lets jump right into our notebook and run all the cells till the first question. For this activity, we will be using an interesting dataset of California Cities which unfortunately has been corrupted due to human error and needs some fixing for the missing values. This dataset contains very important information about 482 cities in the Califorina state with data about its cities, their latitude, logitude, elevation, populations, land area, and water area in different metric units. Lets help clean this data so the scientists can use it for geological research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>latd</th>\n",
       "      <th>longd</th>\n",
       "      <th>elevation_m</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>population_total</th>\n",
       "      <th>area_total_sq_mi</th>\n",
       "      <th>area_land_sq_mi</th>\n",
       "      <th>area_water_sq_mi</th>\n",
       "      <th>area_total_km2</th>\n",
       "      <th>area_land_km2</th>\n",
       "      <th>area_water_km2</th>\n",
       "      <th>area_water_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelanto</td>\n",
       "      <td>34.576111</td>\n",
       "      <td>-117.432778</td>\n",
       "      <td>875.0</td>\n",
       "      <td>2871.0</td>\n",
       "      <td>31765.0</td>\n",
       "      <td>56.027</td>\n",
       "      <td>56.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>145.107</td>\n",
       "      <td>145.062</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AgouraHills</td>\n",
       "      <td>34.153333</td>\n",
       "      <td>-118.761667</td>\n",
       "      <td>281.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>20330.0</td>\n",
       "      <td>7.822</td>\n",
       "      <td>7.793</td>\n",
       "      <td>0.029</td>\n",
       "      <td>20.260</td>\n",
       "      <td>20.184</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alameda</td>\n",
       "      <td>37.756111</td>\n",
       "      <td>-122.274444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>75467.0</td>\n",
       "      <td>22.960</td>\n",
       "      <td>10.611</td>\n",
       "      <td>12.349</td>\n",
       "      <td>59.465</td>\n",
       "      <td>27.482</td>\n",
       "      <td>31.983</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albany</td>\n",
       "      <td>37.886944</td>\n",
       "      <td>-122.297778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>18969.0</td>\n",
       "      <td>5.465</td>\n",
       "      <td>1.788</td>\n",
       "      <td>3.677</td>\n",
       "      <td>14.155</td>\n",
       "      <td>4.632</td>\n",
       "      <td>9.524</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alhambra</td>\n",
       "      <td>34.081944</td>\n",
       "      <td>-118.135000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>83089.0</td>\n",
       "      <td>7.632</td>\n",
       "      <td>7.631</td>\n",
       "      <td>0.001</td>\n",
       "      <td>19.766</td>\n",
       "      <td>19.763</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city       latd       longd  elevation_m  elevation_ft  \\\n",
       "0     Adelanto  34.576111 -117.432778        875.0        2871.0   \n",
       "1  AgouraHills  34.153333 -118.761667        281.0         922.0   \n",
       "2      Alameda  37.756111 -122.274444          NaN          33.0   \n",
       "3       Albany  37.886944 -122.297778          NaN          43.0   \n",
       "4     Alhambra  34.081944 -118.135000        150.0         492.0   \n",
       "\n",
       "   population_total  area_total_sq_mi  area_land_sq_mi  area_water_sq_mi  \\\n",
       "0           31765.0            56.027           56.009             0.018   \n",
       "1           20330.0             7.822            7.793             0.029   \n",
       "2           75467.0            22.960           10.611            12.349   \n",
       "3           18969.0             5.465            1.788             3.677   \n",
       "4           83089.0             7.632            7.631             0.001   \n",
       "\n",
       "   area_total_km2  area_land_km2  area_water_km2  area_water_percent  \n",
       "0         145.107        145.062           0.046                0.03  \n",
       "1          20.260         20.184           0.076                0.37  \n",
       "2          59.465         27.482          31.983                 NaN  \n",
       "3          14.155          4.632           9.524                 NaN  \n",
       "4          19.766         19.763           0.003                 NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('files/california_cities.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city', 'latd', 'longd', 'elevation_m', 'elevation_ft',\n",
       "       'population_total', 'area_total_sq_mi', 'area_land_sq_mi',\n",
       "       'area_water_sq_mi', 'area_total_km2', 'area_land_km2', 'area_water_km2',\n",
       "       'area_water_percent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find out the number of missing values in each column and store them in the variable ``col_missing_values``\n",
    "\n",
    "    Make sure you run all the previous cells. Don't worry if you screw up with the DataFrame! just reload it with the first line of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   0\n",
       "latd                  12\n",
       "longd                 15\n",
       "elevation_m           50\n",
       "elevation_ft          14\n",
       "population_total       2\n",
       "area_total_sq_mi       6\n",
       "area_land_sq_mi        3\n",
       "area_water_sq_mi       5\n",
       "area_total_km2         7\n",
       "area_land_km2          6\n",
       "area_water_km2         6\n",
       "area_water_percent    62\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the total missing values in the whole dataset and store the number in ``df_missing_values``\n",
    "\n",
    "    You must modify the df variable itself. Don't worry if you screw up with the DataFrame! just reload it with the first line of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing_values = df.isnull().sum().sum()\n",
    "df_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Drop the column ``area_water_percent`` as it has the most of its values missing\n",
    "\n",
    "    You have to drop this column permanently as we can not use it for any purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['area_water_percent'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Drop the rows having missing values and store the resulting DataFrame in the variable ``df_narows_dropped``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narows_dropped = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narows_dropped = df[df.notna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narows_dropped = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Drop Rows with More Than 5 Missing Values\n",
    "\n",
    "    Remove rows from the DataFrame where the count of non-null values is less than 5, effectively eliminating rows with more than 5 missing values. Save the resulting DataFrame in the variable ``df_rows_dropped`` and make use of the ``thresh`` parameter for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows_dropped = df.dropna(thresh=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a boolean mask for rows having 5 or fewer missing values\n",
    "row_mask = df.count(axis=1) >= df.shape[1] - 5\n",
    "\n",
    "# Produce a new dataframe composed solely of rows passing the mask\n",
    "df_rows_dropped = df[row_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Remove Columns with Missing Values\n",
    "\n",
    "    Eliminate the columns in the DataFrame that contain missing values. Store the resulting DataFrame in the variable ``df_nacols_dropped``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nacols_dropped = df.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Drop columns with more than 10 missing values and store the resulting DataFrame in the variable ``df_cols_dropped``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                 0\n",
       "latd                12\n",
       "longd               15\n",
       "elevation_m         50\n",
       "elevation_ft        14\n",
       "population_total     2\n",
       "area_total_sq_mi     6\n",
       "area_land_sq_mi      3\n",
       "area_water_sq_mi     5\n",
       "area_total_km2       7\n",
       "area_land_km2        6\n",
       "area_water_km2       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>population_total</th>\n",
       "      <th>area_total_sq_mi</th>\n",
       "      <th>area_land_sq_mi</th>\n",
       "      <th>area_water_sq_mi</th>\n",
       "      <th>area_total_km2</th>\n",
       "      <th>area_land_km2</th>\n",
       "      <th>area_water_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelanto</td>\n",
       "      <td>31765.0</td>\n",
       "      <td>56.027</td>\n",
       "      <td>56.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>145.107</td>\n",
       "      <td>145.062</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AgouraHills</td>\n",
       "      <td>20330.0</td>\n",
       "      <td>7.822</td>\n",
       "      <td>7.793</td>\n",
       "      <td>0.029</td>\n",
       "      <td>20.260</td>\n",
       "      <td>20.184</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alameda</td>\n",
       "      <td>75467.0</td>\n",
       "      <td>22.960</td>\n",
       "      <td>10.611</td>\n",
       "      <td>12.349</td>\n",
       "      <td>59.465</td>\n",
       "      <td>27.482</td>\n",
       "      <td>31.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albany</td>\n",
       "      <td>18969.0</td>\n",
       "      <td>5.465</td>\n",
       "      <td>1.788</td>\n",
       "      <td>3.677</td>\n",
       "      <td>14.155</td>\n",
       "      <td>4.632</td>\n",
       "      <td>9.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alhambra</td>\n",
       "      <td>83089.0</td>\n",
       "      <td>7.632</td>\n",
       "      <td>7.631</td>\n",
       "      <td>0.001</td>\n",
       "      <td>19.766</td>\n",
       "      <td>19.763</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Yountville</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>1.531</td>\n",
       "      <td>1.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.966</td>\n",
       "      <td>3.966</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Yreka</td>\n",
       "      <td>7765.0</td>\n",
       "      <td>10.053</td>\n",
       "      <td>9.980</td>\n",
       "      <td>0.073</td>\n",
       "      <td>26.036</td>\n",
       "      <td>25.847</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>YubaCity</td>\n",
       "      <td>64925.0</td>\n",
       "      <td>14.656</td>\n",
       "      <td>14.578</td>\n",
       "      <td>0.078</td>\n",
       "      <td>37.959</td>\n",
       "      <td>37.758</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Yucaipa</td>\n",
       "      <td>51367.0</td>\n",
       "      <td>27.893</td>\n",
       "      <td>27.888</td>\n",
       "      <td>0.005</td>\n",
       "      <td>72.244</td>\n",
       "      <td>72.231</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>YuccaValley</td>\n",
       "      <td>20700.0</td>\n",
       "      <td>40.015</td>\n",
       "      <td>40.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103.639</td>\n",
       "      <td>103.639</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            city  population_total  area_total_sq_mi  area_land_sq_mi  \\\n",
       "0       Adelanto           31765.0            56.027           56.009   \n",
       "1    AgouraHills           20330.0             7.822            7.793   \n",
       "2        Alameda           75467.0            22.960           10.611   \n",
       "3         Albany           18969.0             5.465            1.788   \n",
       "4       Alhambra           83089.0             7.632            7.631   \n",
       "..           ...               ...               ...              ...   \n",
       "477   Yountville            2933.0             1.531            1.531   \n",
       "478        Yreka            7765.0            10.053            9.980   \n",
       "479     YubaCity           64925.0            14.656           14.578   \n",
       "480      Yucaipa           51367.0            27.893           27.888   \n",
       "481  YuccaValley           20700.0            40.015           40.015   \n",
       "\n",
       "     area_water_sq_mi  area_total_km2  area_land_km2  area_water_km2  \n",
       "0               0.018         145.107        145.062           0.046  \n",
       "1               0.029          20.260         20.184           0.076  \n",
       "2              12.349          59.465         27.482          31.983  \n",
       "3               3.677          14.155          4.632           9.524  \n",
       "4               0.001          19.766         19.763           0.003  \n",
       "..                ...             ...            ...             ...  \n",
       "477             0.000           3.966          3.966           0.000  \n",
       "478             0.073          26.036         25.847           0.188  \n",
       "479             0.078          37.959         37.758           0.201  \n",
       "480             0.005          72.244         72.231           0.013  \n",
       "481             0.000         103.639        103.639           0.000  \n",
       "\n",
       "[482 rows x 8 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[: , missing_values <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols_dropped = df.loc[: , missing_values <= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Fill the 50 missing values in ``elevation_m`` with ``-999``. Store your result in the variable ``filled_elevation_m``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_elevation_m = df['elevation_m'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       875.0\n",
       "1       281.0\n",
       "2      -999.0\n",
       "3      -999.0\n",
       "4       150.0\n",
       "        ...  \n",
       "477      30.0\n",
       "478     787.0\n",
       "479      18.0\n",
       "480     798.0\n",
       "481    1027.0\n",
       "Name: elevation_m, Length: 482, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_elevation_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Fill the 7 missing values in ``area_total_km2`` with the value 0 permanently, store your result in the variable ``filled_area_total``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_area_total = df['area_total_km2'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['area_total_km2'].isna(), 'area_total_km2'] = 0\n",
    "filled_area_total = df['area_total_km2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Fill the missing values of the column ``latd`` using backward filling method and store your result in the variable ``bfill_latd``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfill_latd = df['latd'].bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfill_latd = df['latd'].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Fill the 15 missing values of the column ``longd`` using forwarding filling method and store your result in the variable ``ffill_longd``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffill_longd = df['longd'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Fill the 2 missing values of the column ``population_total`` with the mean of the column and store your result in the variable ``mean_total_population``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_total_population = df['population_total'].fillna(value = df['population_total'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Fill the 5 missing values of the column ``area_water_sq_mi`` with the median value of the column and store your result in the variable ``median_fill``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_fill = df['area_water_sq_mi'].fillna(value = df['area_water_sq_mi'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Fill the 6 missing values of the column ``area_land_km2`` with the ``mode`` value of the column and store your result in the variable ``mode_fill``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_fill = df['area_land_km2'].fillna(df['area_land_km2'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning duplicate data from an Online Retail store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be working with an Online Retail dataset, which contains information about transactions made by an online retailer. The dataset includes details such as the invoice number, stock code, description, quantity, invoice date, unit price, customer ID, and country.\n",
    "\n",
    "Duplicate data is a common issue that can arise in datasets like these, and it's important to be able to identify and remove these duplicates in order to avoid biased or inaccurate analyses. In this practice lab, we will be focusing on identifying and dropping duplicate data using the ``duplicated()`` and ``drop_duplicates()`` methods in pandas.\n",
    "\n",
    "We will start by exploring the dataset and identifying any duplicate rows using the ``duplicated()`` method. Once we have identified the duplicates, we will use the ``drop_duplicates()`` method to remove them from the dataset. Also, the practice includes usage of different parameters that can be used with the ``drop_duplicates()`` method, such as keep and subset.\n",
    "\n",
    "By the end of this practice lab, you should feel comfortable using pandas to identify and remove duplicate data from a dataset, and have a better understanding of how to use the ``duplicated()`` and ``drop_duplicates()`` methods with different parameters to achieve the desired outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read the dataframe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataframe\n",
    "df = pd.read_csv('files/OnlineRetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
       "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
       "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  4335272\n",
      "number of talks:  541909\n",
      "number of features:  8\n"
     ]
    }
   ],
   "source": [
    "#show the number of cells in the dataframe\n",
    "print(\"dataset size: \", df.size)\n",
    "\n",
    "#show the number of records (rows) in the dataframe\n",
    "print(\"number of talks: \", len(df))\n",
    "\n",
    "#show the number of features (coulmns) in the dataframe\n",
    "print(\"number of features: \", len(df.columns)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo       object\n",
       "StockCode       object\n",
       "Description     object\n",
       "Quantity         int64\n",
       "InvoiceDate     object\n",
       "UnitPrice      float64\n",
       "CustomerID     float64\n",
       "Country         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Which of the following parameters is used to only consider certain columns for identifying duplicates and it by default uses all of the columns?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``subset``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Which of the following parameters is used to determine whether to modify the DataFrame rather than creating a new one?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``inplace``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Which of the following parameters takes 'first' as a value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keep`\n",
    "\n",
    "The solution is ``keep``. It means \"determine which duplicates (if any) to keep. For example, ``first`` will drop duplicates except for the first occurrence\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Activities\n",
    "\n",
    "For most activities, make sure you are not modifying the original dataframe. In other words: either don't pass an ``inplace`` parameter, or pass it as ``inplace=False``, so your modifications return a new variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select duplicate rows in a dataframe from the dataset?\n",
    "\n",
    "    Perform the selection and store the results in the variable ``duplicate_rows``.\n",
    "\n",
    "    - Note: use the default parameter of ``keep='first'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the number of duplicate rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find and drop duplicate rows based on InvoiceNo, StockCode, Quantity, and UnitPrice columns\n",
    "\n",
    "    This data contains duplicate orders with the same quantity and unit price, so drop these duplicates.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_without_duplicate_orders``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicate_orders = df.drop(df.loc[df[['InvoiceNo', 'StockCode','Quantity','UnitPrice']].duplicated()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicate_orders = df.drop_duplicates(subset=['InvoiceNo', 'StockCode', 'Quantity', 'UnitPrice'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Drop duplicates while keeping the first non-NaN value based on InvoiceNo, StockCode, and CustomerID columns\n",
    "\n",
    "    As each invoice should have the stock code only one time for each customer and the customer may have different quantities: Drop duplicates while keeping the first non-NaN value.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_keep_first``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541904</th>\n",
       "      <td>581587</td>\n",
       "      <td>22613</td>\n",
       "      <td>PACK OF 20 SPACEBOY NAPKINS</td>\n",
       "      <td>12</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541905</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541906</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541907</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541908</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531225 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0         536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1         536365     71053                  WHITE METAL LANTERN         6   \n",
       "2         536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3         536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4         536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "...          ...       ...                                  ...       ...   \n",
       "541904    581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
       "541905    581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "541906    581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "541907    581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "541908    581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "\n",
       "            InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0        12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
       "1        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "2        12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
       "3        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "4        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "...                 ...        ...         ...             ...  \n",
       "541904  12/9/2011 12:50       0.85     12680.0          France  \n",
       "541905  12/9/2011 12:50       2.10     12680.0          France  \n",
       "541906  12/9/2011 12:50       4.15     12680.0          France  \n",
       "541907  12/9/2011 12:50       4.15     12680.0          France  \n",
       "541908  12/9/2011 12:50       4.95     12680.0          France  \n",
       "\n",
       "[531225 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keep_first = df.drop_duplicates(subset=['InvoiceNo', 'StockCode', 'CustomerID'], keep='first')\n",
    "df_keep_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Drop duplicates while keeping the last order based on StockCode and InvoiceWeekday columns\n",
    "\n",
    "    If you want to show number of unique transactions per weekday and StockCode combination, you will need to drop duplicate stockcode on same day.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_unique_stock_day``.\n",
    "\n",
    "    - Note: use the ``df_weekday`` prepared for you to solve this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert InvoiceDate column to datetime\n",
    "df_weekday = df\n",
    "\n",
    "df_weekday['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Create a new column for weekday\n",
    "df_weekday['InvoiceWeekday'] = df['InvoiceDate'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on StockCode and InvoiceWeekday\n",
    "\n",
    "df_unique_stock_day = df_weekday.drop_duplicates(subset=['StockCode', 'InvoiceWeekday'], keep='last', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Drop all duplicate invoices\n",
    "\n",
    "    Imagine it is black friday and each customer is allowed to buy only at one invoice. So, we need to drop all the duplicate invoices.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_black_friday``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_black_friday = df.drop_duplicates(subset=['InvoiceNo'], keep=False, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Drop duplicate countries while keeping the first row\n",
    "\n",
    "    Imagine we want to know all unique countries in our stock, drop duplicate countries keeping first row.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_unique_countries``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_countries = df.drop_duplicates(subset=['Country'], keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Drop duplicate products while keeping last based on StockCode, Description, and UnitPrice\n",
    "\n",
    "    Imagine we want to know all ordered products in our retail, drop duplicate products based on StockCode, Description, and UnitPrice.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_unique_products``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_products = df.drop_duplicates(subset=['StockCode','Description','UnitPrice'], keep='last', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Drop all duplicate rows based on TotalCost and CustomerID while keeping first\n",
    "\n",
    "    We want to know all unique total costs paid by each different customer, So drop these duplicates.\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_customer_unique_payments``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cost = df\n",
    "# Create a new column for TotalCost\n",
    "df_cost['TotalCost'] = df_cost['Quantity']*df_cost['UnitPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_unique_payments = df_cost.drop_duplicates(subset=['UnitPrice','TotalCost'], keep='first', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Drop all duplicate rows while keeping first\n",
    "\n",
    "    Perform the dropping and store the results in the variable ``df_unique``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Data Cleaning and String Handling with City Bike data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In New York, Citi Bike operates a network of bicycle rental stations with a subscription system. The stations are all over the city and provide a convenient way to get around. Bike rental data is made public in an anonymized form and has been analyzed in various ways. The data has many string columns to be modified before further usage.\n",
    "\n",
    "In this section we will practice how to interact and manipulate string data using Pandas ``.str`` method. We will cover the most common string methods like:\n",
    "\n",
    "- ``.str.capitalize(), .str.lower(), .str.upper(), .str.title()``\n",
    "- ``.str.contains()``\n",
    "- ``.str.count()``\n",
    "- ``.str.endswith(), .str.startswith()``\n",
    "- ``.str.find()``\n",
    "- ``.str.join()``\n",
    "- ``.str.len()``\n",
    "- ``.str.replace()``\n",
    "- ``.str.split()``\n",
    "- ``.str.isdigit() .str.isalpha()``\n",
    "\n",
    "Let's Jump into the practice so that we will clean the data and bring the columns with messy string forms to nice and usable format.\n",
    "\n",
    "### Quick analysis\n",
    "\n",
    "The first few cells in the Notebook read the dataset, display the first few rows (``df.head()``) and show you the column types (``df.info()``). Take your time, exploring the dataset and the columns. Remember that the focus of this project is String Manipulation, so focus on those columns that are of type object.\n",
    "\n",
    "Once you're ready, jump to the next page to start the activities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first name</th>\n",
       "      <th>last name</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>start station id</th>\n",
       "      <th>start station name</th>\n",
       "      <th>end station id</th>\n",
       "      <th>end station name</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>usertype</th>\n",
       "      <th>birth year</th>\n",
       "      <th>gender</th>\n",
       "      <th>emails</th>\n",
       "      <th>pin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>misty</td>\n",
       "      <td>HarRISs</td>\n",
       "      <td>3117</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>18070</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1</td>\n",
       "      <td>osborneangela@example.com</td>\n",
       "      <td>JPCeiknPMw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexis</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>690</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>349</td>\n",
       "      <td>Rivington St &amp; Ridge St</td>\n",
       "      <td>19699</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1</td>\n",
       "      <td>sbutler@example.net</td>\n",
       "      <td>PSxNqDPDWe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jodi</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>727</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>2010</td>\n",
       "      <td>Grand St &amp; Greene St</td>\n",
       "      <td>20953</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>1</td>\n",
       "      <td>egarcia@example.net</td>\n",
       "      <td>pxTMwm166p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>matrick</td>\n",
       "      <td>Evans</td>\n",
       "      <td>698</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>527</td>\n",
       "      <td>E 33 St &amp; 2 Ave</td>\n",
       "      <td>23566</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1</td>\n",
       "      <td>zmoore@example.com</td>\n",
       "      <td>w5dktoeXrq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amanda</td>\n",
       "      <td>DaniEEls</td>\n",
       "      <td>351</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>250</td>\n",
       "      <td>Lafayette St &amp; Jersey St</td>\n",
       "      <td>17545</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>1</td>\n",
       "      <td>derrick11@example.com</td>\n",
       "      <td>8hHtmBR0Tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>Joshua</td>\n",
       "      <td>Donaldson</td>\n",
       "      <td>1225</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>442</td>\n",
       "      <td>W 27 St &amp; 7 Ave</td>\n",
       "      <td>15859</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>kelly26@example.net</td>\n",
       "      <td>NaaPhq7R38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>Deanna</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>939</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>251</td>\n",
       "      <td>Mott St &amp; Prince St</td>\n",
       "      <td>15683</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>seth26@example.net</td>\n",
       "      <td>PWs7OnO37x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>914</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>251</td>\n",
       "      <td>Mott St &amp; Prince St</td>\n",
       "      <td>20219</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>tamara26@example.org</td>\n",
       "      <td>NNEAV3nI8i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>Steven</td>\n",
       "      <td>Riley</td>\n",
       "      <td>1581</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>465</td>\n",
       "      <td>Broadway &amp; W 41 St</td>\n",
       "      <td>23176</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2</td>\n",
       "      <td>carlosstevenson@example.com</td>\n",
       "      <td>dGBYIsZzM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>Hancock</td>\n",
       "      <td>247</td>\n",
       "      <td>301</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>511</td>\n",
       "      <td>E 14 St &amp; Avenue B</td>\n",
       "      <td>23484</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>1</td>\n",
       "      <td>arnolddaniel@example.com</td>\n",
       "      <td>v2VFozNw7y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4212 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     first name  last name  tripduration  start station id start station name  \\\n",
       "0         misty    HarRISs          3117               301  E 2 St & Avenue B   \n",
       "1        Alexis   Thompson           690               301  E 2 St & Avenue B   \n",
       "2          Jodi    Sanders           727               301  E 2 St & Avenue B   \n",
       "3       matrick      Evans           698               301  E 2 St & Avenue B   \n",
       "4        Amanda   DaniEEls           351               301  E 2 St & Avenue B   \n",
       "...         ...        ...           ...               ...                ...   \n",
       "4207     Joshua  Donaldson          1225               301  E 2 St & Avenue B   \n",
       "4208     Deanna     Nelson           939               301  E 2 St & Avenue B   \n",
       "4209   Jennifer    Sanchez           914               301  E 2 St & Avenue B   \n",
       "4210     Steven      Riley          1581               301  E 2 St & Avenue B   \n",
       "4211     Taylor    Hancock           247               301  E 2 St & Avenue B   \n",
       "\n",
       "      end station id          end station name  bikeid    usertype  \\\n",
       "0                301         E 2 St & Avenue B   18070  Subscriber   \n",
       "1                349   Rivington St & Ridge St   19699  Subscriber   \n",
       "2               2010      Grand St & Greene St   20953  Subscriber   \n",
       "3                527           E 33 St & 2 Ave   23566  Subscriber   \n",
       "4                250  Lafayette St & Jersey St   17545  Subscriber   \n",
       "...              ...                       ...     ...         ...   \n",
       "4207             442           W 27 St & 7 Ave   15859    Customer   \n",
       "4208             251       Mott St & Prince St   15683    Customer   \n",
       "4209             251       Mott St & Prince St   20219    Customer   \n",
       "4210             465        Broadway & W 41 St   23176  Subscriber   \n",
       "4211             511        E 14 St & Avenue B   23484  Subscriber   \n",
       "\n",
       "      birth year  gender                       emails         pin  \n",
       "0         1986.0       1    osborneangela@example.com  JPCeiknPMw  \n",
       "1         1985.0       1          sbutler@example.net  PSxNqDPDWe  \n",
       "2         1982.0       1          egarcia@example.net  pxTMwm166p  \n",
       "3         1976.0       1           zmoore@example.com  w5dktoeXrq  \n",
       "4         1959.0       1        derrick11@example.com  8hHtmBR0Tt  \n",
       "...          ...     ...                          ...         ...  \n",
       "4207         NaN       0          kelly26@example.net  NaaPhq7R38  \n",
       "4208         NaN       0           seth26@example.net  PWs7OnO37x  \n",
       "4209         NaN       0         tamara26@example.org  NNEAV3nI8i  \n",
       "4210      1996.0       2  carlosstevenson@example.com  dGBYIsZzM2  \n",
       "4211      1982.0       1     arnolddaniel@example.com  v2VFozNw7y  \n",
       "\n",
       "[4212 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('files/citi-bikes.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4212 entries, 0 to 4211\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   first name          4212 non-null   object \n",
      " 1   last name           4212 non-null   object \n",
      " 2   tripduration        4212 non-null   int64  \n",
      " 3   start station id    4212 non-null   int64  \n",
      " 4   start station name  4212 non-null   object \n",
      " 5   end station id      4212 non-null   int64  \n",
      " 6   end station name    4212 non-null   object \n",
      " 7   bikeid              4212 non-null   int64  \n",
      " 8   usertype            4212 non-null   object \n",
      " 9   birth year          3682 non-null   float64\n",
      " 10  gender              4212 non-null   int64  \n",
      " 11  emails              4212 non-null   object \n",
      " 12  pin                 4212 non-null   object \n",
      "dtypes: float64(1), int64(5), object(7)\n",
      "memory usage: 427.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Capitalize the column first name**\n",
    "\n",
    "    If you explore the DataFrame, you'll see that the column ``first name`` is \"inconsistent\" with its capitalization. Some names are capitalized (Alexis, Jodi), but some others are not (misty, matrick).\n",
    "\n",
    "    Create a new series ``capital_first_name`` that contains the results of the column first name correctly capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_first_name = df['first name'].str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Misty\n",
       "1         Alexis\n",
       "2           Jodi\n",
       "3        Matrick\n",
       "4         Amanda\n",
       "          ...   \n",
       "4207      Joshua\n",
       "4208      Deanna\n",
       "4209    Jennifer\n",
       "4210      Steven\n",
       "4211      Taylor\n",
       "Name: first name, Length: 4212, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_first_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make the Column last name as lower case\n",
    "\n",
    "    Now we can see the Column ``last name`` has very messy format too. Some of the middle letters are capitalized like in ``HarRISs``, ``DaniEEIs`` and some of the first names are not capitalize. So, convert all of them to lower case and store the result in the variable ``lower_last_name``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_last_name = df['last name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         harriss\n",
       "1        thompson\n",
       "2         sanders\n",
       "3           evans\n",
       "4        danieels\n",
       "          ...    \n",
       "4207    donaldson\n",
       "4208       nelson\n",
       "4209      sanchez\n",
       "4210        riley\n",
       "4211      hancock\n",
       "Name: last name, Length: 4212, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_last_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make ``last name`` as Upper case. Store your answer in the variable ``upper_last_name``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_last_name = df['last name'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many users in the Column ``usertype`` are Customer\n",
    "\n",
    "    Lets count all the Customers in the column ``usertype`` and sum them up. Store your sum in the ``customer_counts`` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df = df[df['usertype'] == 'Customer']\n",
    "customer_counts = len(customer_df['emails'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_counts = df['usertype'].str.count('Customer')\n",
    "customer_counts=customer_counts.sum()\n",
    "customer_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_counts = df['usertype'].value_counts()\n",
    "customer_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How many users in the Column ``usertype`` are ``Subscribers``\n",
    "\n",
    "    As you got the total number of Customers in ``usertype`` from the previous question, then also find how many Subscribers are there in total.\n",
    "\n",
    "    You can subtract the number of Customers from the total lenght of the dataframe to find the remain - which are Subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3682"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['usertype'].str.count('Subscriber').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find the words in Column ``pin`` which contain the substring ``lol`` and store your selection in the variable ``word_having_lols``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59        Vtdqzzlolk\n",
       "65          foundlol\n",
       "117       loVulolKDF\n",
       "167       nlolzlVd23\n",
       "2656      6Kolol9toS\n",
       "2660      lolLQigMjv\n",
       "2679        FounDlol\n",
       "2729      26lolbYZ66\n",
       "2744      PlolJhPbGQ\n",
       "2765     UuwlolrTHDW\n",
       "2783      kzlolZJvRk\n",
       "2804      kfJAlolWxZ\n",
       "2828      Eh8lolEwKd\n",
       "2951    145loleGTrpG\n",
       "Name: pin, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_having_lols = df['pin'].str.contains('lol')\n",
    "df['pin'][word_having_lols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Find the names in the Column ``first name`` which start with the letter Z\n",
    "\n",
    "    Find all the names in the ``first name`` column that start with the latter \"Z\". Store the result in the variable ``starts_with_z``.\n",
    "\n",
    "    >***Be careful!*** *It's capital Z, not lowercase z.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233     Zachary\n",
       "348     Zachary\n",
       "431     Zachary\n",
       "651     Zachary\n",
       "847     Zachary\n",
       "1035    Zachary\n",
       "1270    Zachary\n",
       "1366    Zachary\n",
       "1451    Zachary\n",
       "2371    Zachary\n",
       "2508    Zachary\n",
       "2589    Zachary\n",
       "2854    Zachary\n",
       "2964    Zachary\n",
       "3128    Zachary\n",
       "3156    Zachary\n",
       "3535    Zachary\n",
       "3581    Zachary\n",
       "3712    Zachary\n",
       "3814    Zachary\n",
       "Name: first name, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts_with_z = df['first name'].str.startswith('Z')\n",
    "df['first name'][starts_with_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How many first names start with the letter 'Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['first name'][starts_with_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Find the names in the Column ``last name`` which end with 't' and store your result in the variable ``ends_with_t``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67      Lambert\n",
       "101     Elliott\n",
       "111      Knight\n",
       "117     Burnett\n",
       "126       Grant\n",
       "         ...   \n",
       "4107     Hebert\n",
       "4140     Bryant\n",
       "4159    Stewart\n",
       "4181    Garrett\n",
       "4183     Wright\n",
       "Name: last name, Length: 145, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_t = df['last name'].str.endswith('t')\n",
    "df['last name'][ends_with_t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How many Values in the Column last name end with 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ends_with_t.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Concatenate 'bikeid' Entries with Space Separator\n",
    "\n",
    "    Utilize the ``str.join()`` method to concatenate the bikeid values in the ``bikeid`` column, using a space as the separator. Store the result in the variable ``spaced_bikeids``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bikeid']=df['bikeid'].astype(str)\n",
    "\n",
    "spaced_bikeids = df['bikeid'].str.join(sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1 8 0 7 0\n",
       "1       1 9 6 9 9\n",
       "2       2 0 9 5 3\n",
       "3       2 3 5 6 6\n",
       "4       1 7 5 4 5\n",
       "          ...    \n",
       "4207    1 5 8 5 9\n",
       "4208    1 5 6 8 3\n",
       "4209    2 0 2 1 9\n",
       "4210    2 3 1 7 6\n",
       "4211    2 3 4 8 4\n",
       "Name: bikeid, Length: 4212, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaced_bikeids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Create a new Column named ``name length`` having all the lengths of names from the Column ``first name``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name length'] = df['first name'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Find if the Column ``pin`` is alpha numeric or it contains digits only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna pin es numerica? False\n",
      "La columna pin es alfanumerica? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"La columna pin es numerica? {df['pin'].str.isdigit()[0]}\")\n",
    "print(f\"La columna pin es alfanumerica? {df['pin'].str.isalpha()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Verify if the Column ``tripduration`` has any non-numeric values or it contains digits only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna tripduration es numerica? True\n",
      "La columna tripduration es alfanumerica? False\n"
     ]
    }
   ],
   "source": [
    "df['tripduration'] = df['tripduration'].astype(str)\n",
    "\n",
    "print(f\"La columna tripduration es numerica? {df['tripduration'].str.isdigit()[0]}\")\n",
    "print(f\"La columna tripduration es alfanumerica? {df['tripduration'].str.isalpha()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Check if any name in the Column ``first name`` has digit(s) or number(s) in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La columna first name es numerica? False\n",
      "La columna first name es alfanumerica? True\n",
      "La columna first name es decimal? False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4208"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"La columna first name es numerica? {df['first name'].str.isdigit()[0]}\")\n",
    "print(f\"La columna first name es alfanumerica? {df['first name'].str.isalpha()[0]}\")\n",
    "print(f\"La columna first name es decimal? {df['first name'].str.isdecimal()[0]}\")\n",
    "df['first name'].str.isalpha().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Split the emails in the emails column at ``@`` to find the Domain names and store them in the variable ``email_domains``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_domains = df['emails'].str.split('@').str[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Replace the emails having ``.edu`` with ``.org`` and store the output in the variable ``edu_to_org``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace .edu with .org in the email column\n",
    "edu_to_org = df['emails'].str.replace('.edu', '.org',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         osborneangela@example.com\n",
       "1               sbutler@example.net\n",
       "2               egarcia@example.net\n",
       "3                zmoore@example.com\n",
       "4             derrick11@example.com\n",
       "                   ...             \n",
       "4207            kelly26@example.net\n",
       "4208             seth26@example.net\n",
       "4209           tamara26@example.org\n",
       "4210    carlosstevenson@example.com\n",
       "4211       arnolddaniel@example.com\n",
       "Name: emails, Length: 4212, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edu_to_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Replace the numeric and the St values in ``end station name`` Column with ``<space>`` so that we can filter the address without street numbers. Store your result in the variable ``clean_address``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all numeric characters and 'St' with a space\n",
    "clean_address = df['end station name'].str.replace(r'\\d+|St', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             E     & Avenue B\n",
       "1        Rivington   & Ridge  \n",
       "2           Grand   & Greene  \n",
       "3                E     &   Ave\n",
       "4       Lafayette   & Jersey  \n",
       "                 ...          \n",
       "4207             W     &   Ave\n",
       "4208         Mott   & Prince  \n",
       "4209         Mott   & Prince  \n",
       "4210          Broadway & W    \n",
       "4211          E     & Avenue B\n",
       "Name: end station name, Length: 4212, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Strings by Similarity using Levenshtein distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we're going to resolve a very tough problem, which is identifying similar strings that are not completely equal. It's a very common and challenging data-cleaning task, so let's get right into it!\n",
    "\n",
    "We have two CSVs containing company names. The problem is that the company names in these two CSVs don't match. Both have a single column, but if you explore them, you'll immediately spot the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# The new library!\n",
    "from thefuzz import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('files/companies_1.csv')\n",
    "df2 = pd.read_csv('files/companies_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adobe Systems, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adventist Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AECOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aerojet Rockedyne Holdings (GenCorp)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alameda-Contra Costa Transit District</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alaska Community Foundation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alaska Retirement Management Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander &amp; Baldwin, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Allergan, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alyeska Pipeline Service Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  CLIENT\n",
       "0                    Adobe Systems, Inc.\n",
       "1                       Adventist Health\n",
       "2                                  AECOM\n",
       "3   Aerojet Rockedyne Holdings (GenCorp)\n",
       "4  Alameda-Contra Costa Transit District\n",
       "5            Alaska Community Foundation\n",
       "6     Alaska Retirement Management Board\n",
       "7              Alexander & Baldwin, Inc.\n",
       "8                         Allergan, Inc.\n",
       "9       Alyeska Pipeline Service Company"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Firm Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAA Northern California, Nevada &amp; Utah Auto Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACCO Engineered Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adams County Retirement Plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adidas America, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adobe Systems, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Advanced Micro Devices, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AECOM Technology Corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aera Energy LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aerojet Rocketdyne Holdings, Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Agilent Technologies, Inc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Firm Name\n",
       "0  AAA Northern California, Nevada & Utah Auto Ex...\n",
       "1                            ACCO Engineered Systems\n",
       "2                       Adams County Retirement Plan\n",
       "3                               Adidas America, Inc.\n",
       "4                                Adobe Systems, Inc.\n",
       "5                       Advanced Micro Devices, Inc.\n",
       "6                       AECOM Technology Corporation\n",
       "7                                    Aera Energy LLC\n",
       "8                  Aerojet Rocketdyne Holdings, Inc.\n",
       "9                         Agilent Technologies, Inc."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 1)\n",
      "(368, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the company ***AECOM*** appears as ``AECOM`` in CSV1, but as ``AECOM Technology Corporation`` in CSV2.\n",
    "\n",
    "To do so, we're going to use an external library called ``thefuzz`` that contains functionality to compute the Levenshtein distance between two strings. More on this later.\n",
    "\n",
    "Your job will be cleaning this data and matching the columns in both CSVs. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before we can compare the company names, we need to read the data and align the companies for comparison. To do so, we'll need to use the ``itertools.product`` function to create the \"cartesian product\" between the two CSVs. Very briefly, the Cartesian Product of two coll\n",
    "ections ``A`` and ``B``, returns a combination of ALL the elements in both ``A`` and ``B``.\n",
    "\n",
    "Example:\n",
    "```py\n",
    ">>> A = [\"Apple\", \"Alphabet\", \"Microsoft\"]\n",
    ">>> B = [\"MSFT\", \"Alphabet/Google\", \"Apple inc.\"]\n",
    "\n",
    ">>> list(itertools.product(A, B))\n",
    "[('Apple', 'MSFT'),\n",
    " ('Apple', 'Alphabet/Google'),\n",
    " ('Apple', 'Apple inc.'),\n",
    " ('Alphabet', 'MSFT'),\n",
    " ('Alphabet', 'Alphabet/Google'),\n",
    " ('Alphabet', 'Apple inc.'),\n",
    " ('Microsoft', 'MSFT'),\n",
    " ('Microsoft', 'Alphabet/Google'),\n",
    " ('Microsoft', 'Apple inc.')]\n",
    "\n",
    "```\n",
    "As you can see, every element in A is matched with every element in B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Para el lanzamiento de dos dados la combinatoria sería de la siguiente manera\n",
    "\n",
    "dado_1 =  [1,2,3,4,5,6]\n",
    "dado_2 =  [1,2,3,4,5,6]\n",
    "\n",
    "len(list(itertools.product(dado_1, dado_2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTA** ``itertools.product``: Funciona para calcular las posibles combinaciones usando un producto cartesiano de dos series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the ``df`` dataframe containing the product of the two CSVs\n",
    "\n",
    "    We have already read the 2 CSVs into the ``df1`` and ``df2`` variables. Now, use the ``itertools.product`` method to create a resulting dataframe ``df`` that will contain the product of the two CSVs. The columns should be named ``CSV 1`` and ``CSV 2``.\n",
    "\n",
    "    As we have ``266`` rows in ``df1`` and ``368`` in ``df2``, the resulting ``df`` will have ``97,888`` rows (266 * 368), and it'll look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = list(itertools.product(list(df1.iloc[:,0]), list(df2.iloc[:,0])))\n",
    "\n",
    "df = pd.DataFrame(lista, columns=['CSV 1', 'CSV 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = df1['CLIENT'].values\n",
    "csv_2 = df2['Firm Name'].values\n",
    "\n",
    "companies = itertools.product(csv_1, csv_2)\n",
    "\n",
    "df = pd.DataFrame(companies, columns=['CSV 1', 'CSV 2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Levenshtein distance\n",
    "\n",
    "We're going to use the ``partial_ratio`` function from the ``fuzz`` module to compute the \"distance\" between two strings. The result is a number between 0 and ``100``, with ``100`` indicating a \"perfect\" match. Please keep in mind that this method is not perfect, as shown in the examples about ``Microsoft`` in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will learn how to calculate the Levenshtein distance between two strings. Here we will user ``partial_ratio`` function from the fuzz module to compute the \"ratio\" between two strings. The result is a number between 0 and 100, with 100 indicating a \"perfect\" match. Please note that ``partial_ratio`` gives ratio of the shortest string length to the longest string length. For example, if the first string is ``ABC`` and the second string is ``ABDC``, then the ratio will be 4/5 = 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"Apple\", \"Apple Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"Microsoft\", \"Apple Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"Microsoft\", \"MSFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have list of strings, we can calculate the Levenshtein distance between each pair of strings in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [\"Apple\", \"Alphabet\", \"Microsoft\"]\n",
    "B = [\"MSFT\", \"Alphabet/Google\", \"Apple inc.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we combined the two list ``A`` and ``B`` into a list of tuples ``companies`` using product function from ``itertools`` module.\n",
    "\n",
    "Then, we calculated the partial ratio for each pair of strings in the list companies using ``partial_ratio`` function from ``fuzz``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'MSFT'),\n",
       " ('Apple', 'Alphabet/Google'),\n",
       " ('Apple', 'Apple inc.'),\n",
       " ('Alphabet', 'MSFT'),\n",
       " ('Alphabet', 'Alphabet/Google'),\n",
       " ('Alphabet', 'Apple inc.'),\n",
       " ('Microsoft', 'MSFT'),\n",
       " ('Microsoft', 'Alphabet/Google'),\n",
       " ('Microsoft', 'Apple inc.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = list(itertools.product(A, B))\n",
    "companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple > MSFT: 0\n",
      "Apple > Alphabet/Google: 40\n",
      "Apple > Apple inc.: 100\n",
      "Alphabet > MSFT: 0\n",
      "Alphabet > Alphabet/Google: 100\n",
      "Alphabet > Apple inc.: 38\n",
      "Microsoft > MSFT: 25\n",
      "Microsoft > Alphabet/Google: 22\n",
      "Microsoft > Apple inc.: 22\n"
     ]
    }
   ],
   "source": [
    "for c1, c2 in companies:\n",
    "    ratio = fuzz.partial_ratio(c1, c2)\n",
    "    print(f\"{c1} > {c2}: {ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the greater the ratio, the more similar the strings are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new column ``Ratio Score`` that contains the distance for all the rows in df\n",
    "\n",
    "    Now apply the function ``fuzz.partial_ratio`` to all the companies in ``df`` to calculate the distance between them. Store the distance in a new column named ``Ratio Score``. It'll look similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simplest solution, we can use list comprehension to generate the scores from the dataframe ``df``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [fuzz.partial_ratio(c1, c2) for c1, c2 in df.values]\n",
    "df['Ratio Score'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can use the apply method to apply the function to all the rows in df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratio Score'] = df.apply(lambda row: fuzz.partial_ratio(row['CSV 1'], row['CSV 2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adobe Systems, Inc.',\n",
       "       'AAA Northern California, Nevada & Utah Auto Exchange', 26],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many rows have a Ratio score of 90 or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Ratio Score'] >= 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What's the corresponding company in CSV2 to ``AECOM`` in CSV1?\n",
    "\n",
    "    We saw that in ``CSV 1`` there's a company ``AECOM``, what's the corresponding value in ``CSV 2``? The higher the Ratio Score, the more likely it is to be a match. So, use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742    AECOM Technology Corporation\n",
       "Name: CSV 2, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CSV 1'] == 'AECOM') & (df['Ratio Score'] > 90)]['CSV 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What's the corresponding ``CSV 2`` company of Starbucks?\n",
    "\n",
    "    ``CSV 1`` company is ``Starbucks``, what's the corresponding name in ``CSV 2``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77948    Starbucks Corporation\n",
       "Name: CSV 2, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CSV 1'] == 'Starbucks') & (df['Ratio Score'] > 90)]['CSV 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Is there a matching company for ``Pinnacle West Capital Corporation`` in column ``CSV 2``?\n",
    "\n",
    "    Column ``CSV 1`` contains ``Pinnacle West Capital Corporation``, name a matching company in column ``CSV 2`` with a ratio score more than ``90``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV 1</th>\n",
       "      <th>CSV 2</th>\n",
       "      <th>Ratio Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61128</th>\n",
       "      <td>Pinnacle West Capital Corporation</td>\n",
       "      <td>Avista Corporation</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61130</th>\n",
       "      <td>Pinnacle West Capital Corporation</td>\n",
       "      <td>Ball Corporation</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   CSV 1               CSV 2  Ratio Score\n",
       "61128  Pinnacle West Capital Corporation  Avista Corporation           83\n",
       "61130  Pinnacle West Capital Corporation    Ball Corporation           88"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CSV 1'] == 'Pinnacle West Capital Corporation') & (df['Ratio Score'] >= 80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How many matching companies are there for ``County of Los Angeles Deferred Compensation Program``?\n",
    "\n",
    "    ``CSV 1`` contains the ``County of Los Angeles Deferred Compensation Program``. How many matching companies seem to be in ``CSV 2``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV 1</th>\n",
       "      <th>CSV 2</th>\n",
       "      <th>Ratio Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26206</th>\n",
       "      <td>County of Los Angeles Deferred Compensation Pr...</td>\n",
       "      <td>City of Los Angeles Deferred Compensation</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26227</th>\n",
       "      <td>County of Los Angeles Deferred Compensation Pr...</td>\n",
       "      <td>County of Los Angeles Deferred Compensation Pr...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   CSV 1  \\\n",
       "26206  County of Los Angeles Deferred Compensation Pr...   \n",
       "26227  County of Los Angeles Deferred Compensation Pr...   \n",
       "\n",
       "                                                   CSV 2  Ratio Score  \n",
       "26206          City of Los Angeles Deferred Compensation           95  \n",
       "26227  County of Los Angeles Deferred Compensation Pr...          100  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CSV 1'] == 'County of Los Angeles Deferred Compensation Program') & (df['Ratio Score'] > 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Is there a matching company for ``The Queens Health Systems``?\n",
    "\n",
    "    ``CSV 1`` contains ``The Queens Health Systems``, is there a matching in CSV 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV 1</th>\n",
       "      <th>CSV 2</th>\n",
       "      <th>Ratio Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84220</th>\n",
       "      <td>The Queens Health Systems</td>\n",
       "      <td>The Queen's Health Systems</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           CSV 1                       CSV 2  Ratio Score\n",
       "84220  The Queens Health Systems  The Queen's Health Systems           96"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['CSV 1'] == 'The Queens Health Systems') & (df['Ratio Score'] > 90)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String pre-processing for Sentiment Analysis in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll use NLTK, the de-facto and most popular NLP Python library to perform some Sentiment Analysis on a corpus of +22K tweets scraped during the 2022 FIFA world cup.\n",
    "\n",
    "But the objective of this project is not to perform Sentiment Analysis or NLP, as that's outside of the scope of this skill track. The goal of this project is to perform several pre-processing and cleaning tasks to the text of the tweets in order to render them ready for the Sentiment Analysis task.\n",
    "\n",
    "The dataset we're using already includes the classification of Sentiment which was performed with an advanced Deep Learning model based on on [Meta's AI RoBERTa](https://ai.meta.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/).\n",
    "\n",
    "Our goal will be to perform the right pre-processing tasks and then use NLTK's built-in [VADER](https://www.nltk.org/howto/sentiment.html) module to perform a very rudimentary version of Sentiment Analysis, and compare its performance with the original's Sentiment column included in the dataset.\n",
    "\n",
    "Most NLP projects require several cleaning and pre-processing tasks, like tokenization, stemming, lemmatization, etc. Don't worry if you don't understand what they are, we'll explain them as they're required in the project. The important part is to understand that the complexity of NLP usually relies mostly on the pre-processing and cleaning, than the NLP models themselves.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read dataset, but only the ``Tweet`` and ``Sentiment`` columns\n",
    "\n",
    "    Read the data in ``fifa_world_cup_2022_tweets.csv`` into a dataframe, but only the columns ``Tweet`` and ``Sentiment``.\n",
    "\n",
    "    Your ``df`` should look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DVadeus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read dataset, but only the Tweet and Sentiment columns\n",
    "\n",
    "    Read the data in ``fifa_world_cup_2022_tweets.csv`` into a dataframe, but only the columns ``Tweet`` and ``Sentiment``.\n",
    "\n",
    "    Your ``df`` should look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/fifa_world_cup_2022_tweets.csv', usecols =['Tweet', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lowercase the column ``Tweet`` in the new column ``Tweet Lower``\n",
    "\n",
    "    Create a new column Tweet Lower that contains the contents of the ``Tweet`` column, but all lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet Lower'] = df['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are we drinking today @tucantribe \n",
      "@madbears_ \n",
      "@lkinc_algo \n",
      "@al_goanna \n",
      "\n",
      "#worldcup2022 https://t.co/oga3tzvg5h\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['Tweet Lower'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Remove all URLs from ``Tweet Lower`` in ``Tweet Clean``\n",
    "\n",
    "    URLs, hashtags, mentions are mostly useless elements when it comes to sentiment analysis. We'll start by removing all the URLs. Remove all the URLs from Tweet Lower and store your results in Tweet Clean.\n",
    "\n",
    "    Warning! Don't forget to remove any leading or trailing whitespaces. For example, if you remove the URL from the following tweet:\n",
    "\n",
    "    ```py\n",
    "    what are we drinking today @tucantribe \n",
    "    @madbears_ \n",
    "    @lkinc_algo \n",
    "    @al_goanna \n",
    "    \n",
    "    #worldcup2022 https://t.co/oga3tzvg5h\n",
    "    ```\n",
    "\n",
    "    The result should be:\n",
    "    \n",
    "    ``` py\n",
    "    \"\"\"what are we drinking today @tucantribe \n",
    "    @madbears_ \n",
    "    @lkinc_algo \n",
    "    @al_goanna \n",
    "\n",
    "    #worldcup2022\"\"\"\n",
    "    ```\n",
    "    \n",
    "    Without a trailing space after the #worldcup2022 hashtag.\n",
    "\n",
    "    > *Use this pattern for matching the url: ``https?:\\/\\/[^\\s]*``*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet Clean'] = df.apply(lambda row: re.sub('https?:\\/\\/[^\\s]*', \"\", row['Tweet Lower'], flags=re.MULTILINE).strip(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are we drinking today @tucantribe \\n@madbears_ \\n@lkinc_algo \\n@al_goanna \\n\\n#worldcup2022'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet Clean'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Remove username mentions in ``Tweet Clean``\n",
    "\n",
    "    Still in ``Tweet Clean``, remove any twitter mentions (in the form ``@datawars_io``). In this case, we're modifying the original column Tweet Clean, so if you make a mistake, you'll have to re-run your previous code and start over.\n",
    "\n",
    "    Remember to strip any trailing or leading whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet Clean'] = df['Tweet Clean'].str.replace('@[\\w]*', '',regex = True, flags = re.MULTILINE).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove Hashtags from Tweet Clean\n",
    "\n",
    "    Still in Tweet Clean, remove any hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tweet Clean'] = df['Tweet Clean'].str.replace('#[\\w]*', '',regex = True, flags = re.MULTILINE).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Tweets\n",
    "\n",
    "6. Create the list ``tokenized_tweets`` by applying the function ``word_tokenize`` to the values of the column ``Tweet Clean``\n",
    "\n",
    "    We'll now start using the ``nltk`` module. Don't worry if you've never used it before, as these are all simple functions that don't require an NLP background.\n",
    "\n",
    "    We'll start by \"tokenizing\" the tweets. Tokenizing means basically splitting a corpus of text into different words or tokens.\n",
    "\n",
    "    Your task is to use the ``word_tokenize`` function to create a list of tweet tokens and store the result in ``tokenized_tweets``. This means that ``tokenized_tweets`` is a list of lists, a list of tokens in the following form:\n",
    "    ```py\n",
    "    [\n",
    "        ['what', 'are', 'we', 'drinking', 'today'], # tweet\n",
    "        ['worth', 'reading', 'while', 'watching'],  # tweet\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = [word_tokenize(w) for w in df['Tweet Clean'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filter stop words\n",
    "\n",
    "    Stop words are words that don't contribute much to the meaning of a sentence, like conjunctions (\"for\", \"and\") or the word \"the\", \"a\", etc. The ``nltk`` module contains stop words for english, that we can get with ``stopwords.words('english')``.\n",
    "\n",
    "    Your task is to remove any stop words from the tokens you have previously generated. Store your results in the variable ``filtered_tokenized_tweets``, which continues to be a list of lists, but with the stop words filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DVadeus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenized_tweets = [[w for w in tweet if w not in stop_words_english] for tweet in tokenized_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Sentiment Analysis\n",
    "\n",
    "We'll now start the process of sentiment analysis using the builtin ``SentimentIntensityAnalyzer`` NLTK class, also known as VADER.\n",
    "\n",
    "Vader works well for the basic tasks, for example this will be classified mostly as a positive sentence:\n",
    "\n",
    "```py\n",
    ">>> analyzer.polarity_scores(\"DataWars is awesome! I love it so much!\")\n",
    "{'neg': 0.0, 'neu': 0.36, 'pos': 0.64, 'compound': 0.8715}\n",
    "```\n",
    "\n",
    "But it can't handle things like sarcasm or rethorical questions. For example, sarcasm is completely lost to the analyzer, this is clearly a negative sentence:\n",
    "\n",
    "```py\n",
    ">>> analyzer.polarity_scores(\"Yeah, R is sure a great programming language. If you are 80 years old\")\n",
    "{'neg': 0.0, 'neu': 0.511, 'pos': 0.489, 'compound': 0.8225}\n",
    "```\n",
    "\n",
    "While the following one, which is not as extreme as the previous one, could also be arguably classified as negative. Instead, it's found to be completely neutral:\n",
    "```py\n",
    ">>> analyzer.polarity_scores(\"I can't believe how people still program in R in 2019, what is going on?\")\n",
    "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "```\n",
    "\n",
    "Don't worry too much about the contents of the resulting dictionary, we'll explain them in the following activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Glue all the tweets back again\n",
    "\n",
    "    Use a single space to concat the tokens that we have preprocessed in our previous tasks and build the tweet again. Store your results in the variable ``cleaned_tweets``. In this case, it'll no longer be a list of lists, but a list of strings, the tweets we have assembled again, and it'll look something like:\n",
    "    ```py\n",
    "    ['drinking today',\n",
    "    'amazing launch video . shows much face canada men ’ national team changed since last world cup entry 1986. ’ wait see boys action ! canada : fifa world cup opening video',\n",
    "    'worth reading watching']\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "for tweet in filtered_tokenized_tweets:\n",
    "    word = ''\n",
    "    for t in tweet:\n",
    "        word += t+' '\n",
    "    cleaned_tweets.append(word.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drinking today',\n",
       " 'amazing launch video . shows much face canada men ’ national team changed since last world cup entry 1986. ’ wait see boys action ! canada : fifa world cup opening video',\n",
       " 'worth reading watching',\n",
       " 'golden maknae shinning bright',\n",
       " 'bbc cares much human rights , homosexual rights , women rights say opening ceremony ? ? saying opening ceremony ? ? bbc censor opening ceremony ? ?']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweets = [' '.join(tweet) for tweet in filtered_tokenized_tweets]\n",
    "cleaned_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Apply VADER to all the words\n",
    "\n",
    "    Use the ``analyzer.polarity_scores`` method to perform sentiment analysis on all the tweets in ``cleaned_tweets``. Store the list of results in the variable ``tweet_sentiment_scores``.\n",
    "\n",
    "    As we mentioned before, this requires just a method invocation:\n",
    "    ```py\n",
    "    >>> analyzer.polarity_scores(YOUR_TWEET)\n",
    "    ```\n",
    "    Your ``tweet_sentiment_scores`` variable will look something like:\n",
    "    ```py\n",
    "    [\n",
    "        {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
    "        {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.6239},\n",
    "        {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.2263},\n",
    "        ...\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DVadeus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_scores = [analyzer.polarity_scores(tweet) for tweet in cleaned_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       " {'neg': 0.0, 'neu': 0.864, 'pos': 0.136, 'compound': 0.6239},\n",
       " {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.2263},\n",
       " {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404},\n",
       " {'neg': 0.13, 'neu': 0.739, 'pos': 0.13, 'compound': 0.0}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_sentiment_scores[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Calculate the sentiment of each tweet based on the following rule....\n",
    "\n",
    "    The result of ``analyzer.polarity_scores`` is a dictionary with several keys:\n",
    "    \n",
    "    ```py\n",
    "    >>> analyzer.polarity_scores(\"DataWars is awesome! I love it so much!\")\n",
    "    {'neg': 0.0, 'neu': 0.36, 'pos': 0.64, 'compound': 0.8715}\n",
    "    ```\n",
    "    \n",
    "    The ``neg``, ``neu`` and ``pos`` keys represent the proportions of the text that fall in each category (Negative, Neutral and Positive). They add up to 1.\n",
    "\n",
    "    But the key that we're really interested in is ``compound``, which is a weighted composite score that has been normalized between -1 (most extreme negative) and +1 (most extreme positive). In this case, the ``compound`` score of 0.8715 indicates a very high positive sentiment.\n",
    "\n",
    "    The general rule of thumb for interpreting the ``compound`` score is:\n",
    "\n",
    "    - Positive sentiment: ``compound`` score > 0.05\n",
    "    - Neutral sentiment: ``compound`` score between -0.05 and 0.05\n",
    "    - Negative sentiment: ``compound`` score < -0.05\n",
    "    \n",
    "    Calculate the sentiment of each score and store it in the variable ``tweet_sentiment_results`` that should look something like: ``['neutral', 'positive', 'positive', ...]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_results = ['positive' if score['compound'] > 0.05 else 'negative' if score['compound'] < -0.05 else 'neutral' for score in tweet_sentiment_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(res):\n",
    "    comp = res['compound']\n",
    "    if comp > .05:\n",
    "        return \"positive\"\n",
    "    elif comp < -.05:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_results = [get_sentiment(score) for score in tweet_sentiment_scores]\n",
    "tweet_sentiment_results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Delete the columns ``Tweet Lower`` and ``Tweet Clean`` from and add the new column ``Calculated Sentiment``\n",
    "\n",
    "    Remove the columns we previously used (``Tweet Lower``, ``Tweet Clean``) and create a new one named ``Calculated Sentiment`` with the results of ``tweet_sentiment_results``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Tweet Lower', 'Tweet Clean'], inplace = True)\n",
    "df['Calculated Sentiment'] = tweet_sentiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How many tweets were incorrectly classified?\n",
    "\n",
    "    Assuming the column ``Sentiment`` had the correct sentiment, how many did we classified erroneously in our ``Calculated Sentiment`` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Calculated Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Look like a only me and the Jamaican football ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Really? Football on a Monday morning at 9 and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>As the World Cup starts in Qatar, it’s Black A...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Qatar tried to help its useless soccer team 5...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>#WorldCup2022 is starting tonight\\nAnd with th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>The World Cup starts TODAY!\\n\\n#Qatar2022 #Foo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Here We go World cup 2022 #WorldCup2022</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>Anderlecht confirms former Viborg FF's Jesper ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>How to buy $SOT on PinkSale?🤔\\n\\nHave you been...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9508 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Sentiment  \\\n",
       "4      If the BBC cares so much about human rights, h...  negative   \n",
       "6      Look like a only me and the Jamaican football ...   neutral   \n",
       "7      Really? Football on a Monday morning at 9 and ...  negative   \n",
       "8      As the World Cup starts in Qatar, it’s Black A...  positive   \n",
       "10     #Qatar tried to help its useless soccer team 5...  negative   \n",
       "...                                                  ...       ...   \n",
       "22514  #WorldCup2022 is starting tonight\\nAnd with th...  negative   \n",
       "22517  The World Cup starts TODAY!\\n\\n#Qatar2022 #Foo...  positive   \n",
       "22519            Here We go World cup 2022 #WorldCup2022  positive   \n",
       "22520  Anderlecht confirms former Viborg FF's Jesper ...   neutral   \n",
       "22523  How to buy $SOT on PinkSale?🤔\\n\\nHave you been...   neutral   \n",
       "\n",
       "      Calculated Sentiment  \n",
       "4                  neutral  \n",
       "6                 positive  \n",
       "7                  neutral  \n",
       "8                 negative  \n",
       "10                positive  \n",
       "...                    ...  \n",
       "22514             positive  \n",
       "22517              neutral  \n",
       "22519              neutral  \n",
       "22520             positive  \n",
       "22523             negative  \n",
       "\n",
       "[9508 rows x 3 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Sentiment'] != df['Calculated Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. How many Negative tweets were incorrectly classified (either as ``Neutral`` or ``Positive``)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Calculated Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Really? Football on a Monday morning at 9 and ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#Qatar tried to help its useless soccer team 5...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>If the BBC cares so much about human rights, g...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>With your looks, you couldn't even get with a ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22448</th>\n",
       "      <td>If the Royal Family can pressure the Supreme C...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22449</th>\n",
       "      <td>I can’t believe the World Cup starts tomorrow....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22455</th>\n",
       "      <td>The World Cup should have started today so the...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>#Qatar doing to the #WorldCup2022 what @elonmu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>#WorldCup2022 is starting tonight\\nAnd with th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3026 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Sentiment  \\\n",
       "4      If the BBC cares so much about human rights, h...  negative   \n",
       "7      Really? Football on a Monday morning at 9 and ...  negative   \n",
       "10     #Qatar tried to help its useless soccer team 5...  negative   \n",
       "36     If the BBC cares so much about human rights, g...  negative   \n",
       "38     With your looks, you couldn't even get with a ...  negative   \n",
       "...                                                  ...       ...   \n",
       "22448  If the Royal Family can pressure the Supreme C...  negative   \n",
       "22449  I can’t believe the World Cup starts tomorrow....  negative   \n",
       "22455  The World Cup should have started today so the...  negative   \n",
       "22496  #Qatar doing to the #WorldCup2022 what @elonmu...  negative   \n",
       "22514  #WorldCup2022 is starting tonight\\nAnd with th...  negative   \n",
       "\n",
       "      Calculated Sentiment  \n",
       "4                  neutral  \n",
       "7                  neutral  \n",
       "10                positive  \n",
       "36                 neutral  \n",
       "38                 neutral  \n",
       "...                    ...  \n",
       "22448             positive  \n",
       "22449              neutral  \n",
       "22455              neutral  \n",
       "22496             positive  \n",
       "22514             positive  \n",
       "\n",
       "[3026 rows x 3 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['Sentiment'] == 'negative') & (df['Sentiment'] != df['Calculated Sentiment'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing identifyin and dealing with invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be working with a Human Resources dataset containing information about employees from a fictional company. The dataset includes details such as employee ID, date of birth, gender, marital status, job title, department, salary, performance score, and other columns.\n",
    "\n",
    "Invalid or missing values are common issues that can arise in datasets like these, and it's important to be able to identify and clean these invalid values to avoid biased or inaccurate analyses. In this lab, we will be focusing on cleaning invalid values in selected columns using different methods in pandas.\n",
    "\n",
    "We will start by exploring the dataset and identifying any invalid or missing values in the selected columns using different methods to clean the data, based on the data type. We will focus on 4 different types: - Invalid numeric column - Invalid categories - Invalid datetimes - Invalid patterns in strings\n",
    "\n",
    "By the end of this lab, you should feel comfortable using pandas to clean invalid values in a dataset, and have a better understanding of how to use different methods and techniques to achieve the desired outcome. The lab includes basic activities and advanced activities, focusing on different aspects of cleaning invalid values in the Human Resources dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/HRDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee_Name</th>\n",
       "      <th>EmpID</th>\n",
       "      <th>DeptID</th>\n",
       "      <th>Salary</th>\n",
       "      <th>PositionID</th>\n",
       "      <th>Position</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>DOB</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>RecruitmentSource</th>\n",
       "      <th>PerformanceScore</th>\n",
       "      <th>EngagementSurvey</th>\n",
       "      <th>EmpSatisfaction</th>\n",
       "      <th>SpecialProjectsCount</th>\n",
       "      <th>LastPerformanceReview_Date</th>\n",
       "      <th>DaysLateLast30</th>\n",
       "      <th>Absences</th>\n",
       "      <th>Email</th>\n",
       "      <th>Phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adinolfi, Wilson  K</td>\n",
       "      <td>10026</td>\n",
       "      <td>5</td>\n",
       "      <td>62506</td>\n",
       "      <td>19</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>MA</td>\n",
       "      <td>1960</td>\n",
       "      <td>07/10/83</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Exceeds</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1/17/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adinolfi@company.com</td>\n",
       "      <td>+62 199-807-6651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ait Sidi, Karthikeyan</td>\n",
       "      <td>10084</td>\n",
       "      <td>3</td>\n",
       "      <td>104437</td>\n",
       "      <td>27</td>\n",
       "      <td>Sr. DBA</td>\n",
       "      <td>MA</td>\n",
       "      <td>2148</td>\n",
       "      <td>05/05/75</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>4.96</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>Ait Sidi@company.com</td>\n",
       "      <td>+358 128-157-9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akinkuolie, Sarah</td>\n",
       "      <td>10196</td>\n",
       "      <td>5</td>\n",
       "      <td>64955</td>\n",
       "      <td>20</td>\n",
       "      <td>Production Technician II</td>\n",
       "      <td>MA</td>\n",
       "      <td>1810</td>\n",
       "      <td>09/19/88</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5/15/2012</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Akinkuolie@company.com</td>\n",
       "      <td>+474 737-382-6683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alagbe,Trina</td>\n",
       "      <td>10088</td>\n",
       "      <td>5</td>\n",
       "      <td>64991</td>\n",
       "      <td>19</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>MA</td>\n",
       "      <td>1886</td>\n",
       "      <td>09/27/88</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>4.84</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1/3/2019</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Alagbe@company.com</td>\n",
       "      <td>+806 538-458-2517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anderson, Carol</td>\n",
       "      <td>10069</td>\n",
       "      <td>5</td>\n",
       "      <td>50825</td>\n",
       "      <td>19</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>MA</td>\n",
       "      <td>2169</td>\n",
       "      <td>09/08/89</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>Google Search</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2/1/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Anderson@company.com</td>\n",
       "      <td>+148 658-824-9500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Employee_Name  EmpID  DeptID  Salary  PositionID  \\\n",
       "0       Adinolfi, Wilson  K  10026       5   62506          19   \n",
       "1  Ait Sidi, Karthikeyan     10084       3  104437          27   \n",
       "2         Akinkuolie, Sarah  10196       5   64955          20   \n",
       "3              Alagbe,Trina  10088       5   64991          19   \n",
       "4          Anderson, Carol   10069       5   50825          19   \n",
       "\n",
       "                   Position State   Zip       DOB Sex  ... RecruitmentSource  \\\n",
       "0   Production Technician I    MA  1960  07/10/83   M  ...          LinkedIn   \n",
       "1                   Sr. DBA    MA  2148  05/05/75   M  ...            Indeed   \n",
       "2  Production Technician II    MA  1810  09/19/88   F  ...          LinkedIn   \n",
       "3   Production Technician I    MA  1886  09/27/88   F  ...            Indeed   \n",
       "4   Production Technician I    MA  2169  09/08/89   F  ...     Google Search   \n",
       "\n",
       "  PerformanceScore EngagementSurvey EmpSatisfaction SpecialProjectsCount  \\\n",
       "0          Exceeds             4.60               5                    0   \n",
       "1      Fully Meets             4.96               3                    6   \n",
       "2      Fully Meets             3.02               3                    0   \n",
       "3      Fully Meets             4.84               5                    0   \n",
       "4      Fully Meets             5.00               4                    0   \n",
       "\n",
       "  LastPerformanceReview_Date DaysLateLast30 Absences                   Email  \\\n",
       "0                  1/17/2019              0        1    Adinolfi@company.com   \n",
       "1                  2/24/2016              0       17    Ait Sidi@company.com   \n",
       "2                  5/15/2012              0        3  Akinkuolie@company.com   \n",
       "3                   1/3/2019              0       15      Alagbe@company.com   \n",
       "4                   2/1/2016              0        2    Anderson@company.com   \n",
       "\n",
       "               Phone  \n",
       "0   +62 199-807-6651  \n",
       "1  +358 128-157-9500  \n",
       "2  +474 737-382-6683  \n",
       "3  +806 538-458-2517  \n",
       "4  +148 658-824-9500  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Employee_Name</th>\n",
       "      <td>Adinolfi, Wilson  K</td>\n",
       "      <td>Ait Sidi, Karthikeyan</td>\n",
       "      <td>Akinkuolie, Sarah</td>\n",
       "      <td>Alagbe,Trina</td>\n",
       "      <td>Anderson, Carol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EmpID</th>\n",
       "      <td>10026</td>\n",
       "      <td>10084</td>\n",
       "      <td>10196</td>\n",
       "      <td>10088</td>\n",
       "      <td>10069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeptID</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Salary</th>\n",
       "      <td>62506</td>\n",
       "      <td>104437</td>\n",
       "      <td>64955</td>\n",
       "      <td>64991</td>\n",
       "      <td>50825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PositionID</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Position</th>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Sr. DBA</td>\n",
       "      <td>Production Technician II</td>\n",
       "      <td>Production Technician I</td>\n",
       "      <td>Production Technician I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zip</th>\n",
       "      <td>1960</td>\n",
       "      <td>2148</td>\n",
       "      <td>1810</td>\n",
       "      <td>1886</td>\n",
       "      <td>2169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOB</th>\n",
       "      <td>07/10/83</td>\n",
       "      <td>05/05/75</td>\n",
       "      <td>09/19/88</td>\n",
       "      <td>09/27/88</td>\n",
       "      <td>09/08/89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalDesc</th>\n",
       "      <td>Single</td>\n",
       "      <td>Married</td>\n",
       "      <td>Married</td>\n",
       "      <td>Married</td>\n",
       "      <td>Divorced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CitizenDesc</th>\n",
       "      <td>US Citizen</td>\n",
       "      <td>US Citizen</td>\n",
       "      <td>US Citizen</td>\n",
       "      <td>US Citizen</td>\n",
       "      <td>US Citizen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HispanicLatino</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RaceDesc</th>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateofHire</th>\n",
       "      <td>7/5/2011</td>\n",
       "      <td>3/30/2015</td>\n",
       "      <td>7/5/2011</td>\n",
       "      <td>1/7/2008</td>\n",
       "      <td>7/11/2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateofTermination</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>9/24/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/6/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TermReason</th>\n",
       "      <td>N/A-StillEmployed</td>\n",
       "      <td>career change</td>\n",
       "      <td>hours</td>\n",
       "      <td>N/A-StillEmployed</td>\n",
       "      <td>return to school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EmploymentStatus</th>\n",
       "      <td>Active</td>\n",
       "      <td>Voluntarily Terminated</td>\n",
       "      <td>Voluntarily Terminated</td>\n",
       "      <td>Active</td>\n",
       "      <td>Voluntarily Terminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department</th>\n",
       "      <td>Production</td>\n",
       "      <td>IT/IS</td>\n",
       "      <td>Production</td>\n",
       "      <td>Production</td>\n",
       "      <td>Production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ManagerName</th>\n",
       "      <td>Michael Albert</td>\n",
       "      <td>Simon Roup</td>\n",
       "      <td>Kissy Sullivan</td>\n",
       "      <td>Elijiah Gray</td>\n",
       "      <td>Webster Butler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ManagerID</th>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RecruitmentSource</th>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Google Search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerformanceScore</th>\n",
       "      <td>Exceeds</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>Fully Meets</td>\n",
       "      <td>Fully Meets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EngagementSurvey</th>\n",
       "      <td>4.6</td>\n",
       "      <td>4.96</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.84</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EmpSatisfaction</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpecialProjectsCount</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastPerformanceReview_Date</th>\n",
       "      <td>1/17/2019</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>5/15/2012</td>\n",
       "      <td>1/3/2019</td>\n",
       "      <td>2/1/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DaysLateLast30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Absences</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Email</th>\n",
       "      <td>Adinolfi@company.com</td>\n",
       "      <td>Ait Sidi@company.com</td>\n",
       "      <td>Akinkuolie@company.com</td>\n",
       "      <td>Alagbe@company.com</td>\n",
       "      <td>Anderson@company.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phone</th>\n",
       "      <td>+62 199-807-6651</td>\n",
       "      <td>+358 128-157-9500</td>\n",
       "      <td>+474 737-382-6683</td>\n",
       "      <td>+806 538-458-2517</td>\n",
       "      <td>+148 658-824-9500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  0                         1  \\\n",
       "Employee_Name                   Adinolfi, Wilson  K  Ait Sidi, Karthikeyan      \n",
       "EmpID                                         10026                     10084   \n",
       "DeptID                                            5                         3   \n",
       "Salary                                        62506                    104437   \n",
       "PositionID                                       19                        27   \n",
       "Position                    Production Technician I                   Sr. DBA   \n",
       "State                                            MA                        MA   \n",
       "Zip                                            1960                      2148   \n",
       "DOB                                        07/10/83                  05/05/75   \n",
       "Sex                                               M                         M   \n",
       "MaritalDesc                                  Single                   Married   \n",
       "CitizenDesc                              US Citizen                US Citizen   \n",
       "HispanicLatino                                   No                        No   \n",
       "RaceDesc                                      White                     White   \n",
       "DateofHire                                 7/5/2011                 3/30/2015   \n",
       "DateofTermination                               NaN                 6/16/2016   \n",
       "TermReason                        N/A-StillEmployed             career change   \n",
       "EmploymentStatus                             Active    Voluntarily Terminated   \n",
       "Department                        Production                            IT/IS   \n",
       "ManagerName                          Michael Albert                Simon Roup   \n",
       "ManagerID                                      22.0                       4.0   \n",
       "RecruitmentSource                          LinkedIn                    Indeed   \n",
       "PerformanceScore                            Exceeds               Fully Meets   \n",
       "EngagementSurvey                                4.6                      4.96   \n",
       "EmpSatisfaction                                   5                         3   \n",
       "SpecialProjectsCount                              0                         6   \n",
       "LastPerformanceReview_Date                1/17/2019                 2/24/2016   \n",
       "DaysLateLast30                                    0                         0   \n",
       "Absences                                          1                        17   \n",
       "Email                          Adinolfi@company.com      Ait Sidi@company.com   \n",
       "Phone                              +62 199-807-6651         +358 128-157-9500   \n",
       "\n",
       "                                                   2                        3  \\\n",
       "Employee_Name                      Akinkuolie, Sarah             Alagbe,Trina   \n",
       "EmpID                                          10196                    10088   \n",
       "DeptID                                             5                        5   \n",
       "Salary                                         64955                    64991   \n",
       "PositionID                                        20                       19   \n",
       "Position                    Production Technician II  Production Technician I   \n",
       "State                                             MA                       MA   \n",
       "Zip                                             1810                     1886   \n",
       "DOB                                         09/19/88                 09/27/88   \n",
       "Sex                                                F                        F   \n",
       "MaritalDesc                                  Married                  Married   \n",
       "CitizenDesc                               US Citizen               US Citizen   \n",
       "HispanicLatino                                    No                       No   \n",
       "RaceDesc                                       White                    White   \n",
       "DateofHire                                  7/5/2011                 1/7/2008   \n",
       "DateofTermination                          9/24/2012                      NaN   \n",
       "TermReason                                     hours        N/A-StillEmployed   \n",
       "EmploymentStatus              Voluntarily Terminated                   Active   \n",
       "Department                         Production               Production          \n",
       "ManagerName                           Kissy Sullivan             Elijiah Gray   \n",
       "ManagerID                                       20.0                     16.0   \n",
       "RecruitmentSource                           LinkedIn                   Indeed   \n",
       "PerformanceScore                         Fully Meets              Fully Meets   \n",
       "EngagementSurvey                                3.02                     4.84   \n",
       "EmpSatisfaction                                    3                        5   \n",
       "SpecialProjectsCount                               0                        0   \n",
       "LastPerformanceReview_Date                 5/15/2012                 1/3/2019   \n",
       "DaysLateLast30                                     0                        0   \n",
       "Absences                                           3                       15   \n",
       "Email                         Akinkuolie@company.com       Alagbe@company.com   \n",
       "Phone                              +474 737-382-6683        +806 538-458-2517   \n",
       "\n",
       "                                                  4  \n",
       "Employee_Name                      Anderson, Carol   \n",
       "EmpID                                         10069  \n",
       "DeptID                                            5  \n",
       "Salary                                        50825  \n",
       "PositionID                                       19  \n",
       "Position                    Production Technician I  \n",
       "State                                            MA  \n",
       "Zip                                            2169  \n",
       "DOB                                        09/08/89  \n",
       "Sex                                               F  \n",
       "MaritalDesc                                Divorced  \n",
       "CitizenDesc                              US Citizen  \n",
       "HispanicLatino                                   No  \n",
       "RaceDesc                                      White  \n",
       "DateofHire                                7/11/2011  \n",
       "DateofTermination                          9/6/2016  \n",
       "TermReason                         return to school  \n",
       "EmploymentStatus             Voluntarily Terminated  \n",
       "Department                        Production         \n",
       "ManagerName                          Webster Butler  \n",
       "ManagerID                                        No  \n",
       "RecruitmentSource                     Google Search  \n",
       "PerformanceScore                        Fully Meets  \n",
       "EngagementSurvey                                5.0  \n",
       "EmpSatisfaction                                   4  \n",
       "SpecialProjectsCount                              0  \n",
       "LastPerformanceReview_Date                 2/1/2016  \n",
       "DaysLateLast30                                    0  \n",
       "Absences                                          2  \n",
       "Email                          Anderson@company.com  \n",
       "Phone                             +148 658-824-9500  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size:  9641\n",
      "number of employees:  311\n",
      "number of features:  31\n"
     ]
    }
   ],
   "source": [
    "#show the number of cells in the dataframe\n",
    "print(\"dataset size: \", df.size)\n",
    "\n",
    "#show the number of records (rows) in the dataframe\n",
    "print(\"number of employees: \", len(df))\n",
    "\n",
    "#show the number of features (columns) in the dataframe\n",
    "print(\"number of features: \", len(df.columns)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Clean the column Salary by removing invalid values\n",
    "\n",
    "    Invalid values are defined as any value that is not an integer.\n",
    "\n",
    "    Perform the selection of valid values and store them in column ``Salary_Fixed`` while invalid values should be ``NaN``. Then select invalid values and store the results in the variable ``df_invalid_salaries``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_Fixed'] = df[df['Salary'].str.isnumeric()]['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalid_salaries = df.loc[df['Salary_Fixed'].isna(), ['Salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>oFtFtkls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>HEbELLmj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>DMZIQhGz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>XSfelyZh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>RxhGvEBX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>xqCStzCF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>gHNxOVWk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>WXrOyJEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>OAWNTcGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>dvCsKwyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>ZGEoVKAo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>KFpwkWHX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Salary\n",
       "22   oFtFtkls\n",
       "41   HEbELLmj\n",
       "62   DMZIQhGz\n",
       "71   XSfelyZh\n",
       "117  RxhGvEBX\n",
       "138  xqCStzCF\n",
       "165  gHNxOVWk\n",
       "216  WXrOyJEO\n",
       "220  OAWNTcGQ\n",
       "259  dvCsKwyl\n",
       "291  ZGEoVKAo\n",
       "304  KFpwkWHX"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invalid_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clean the column ``Zip`` by removing invalid values\n",
    "\n",
    "    Invalid values are defined as any value that is not an integer.\n",
    "\n",
    "    Perform the selection of valid values and store them in column ``Zip_Fixed`` while invalid values should be ``NaN``. Then select invalid values and store the results in the variable ``df_invalid_zip``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311 entries, 0 to 310\n",
      "Data columns (total 32 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Employee_Name               311 non-null    object \n",
      " 1   EmpID                       311 non-null    int64  \n",
      " 2   DeptID                      311 non-null    int64  \n",
      " 3   Salary                      311 non-null    object \n",
      " 4   PositionID                  311 non-null    int64  \n",
      " 5   Position                    311 non-null    object \n",
      " 6   State                       311 non-null    object \n",
      " 7   Zip                         311 non-null    object \n",
      " 8   DOB                         310 non-null    object \n",
      " 9   Sex                         311 non-null    object \n",
      " 10  MaritalDesc                 311 non-null    object \n",
      " 11  CitizenDesc                 311 non-null    object \n",
      " 12  HispanicLatino              311 non-null    object \n",
      " 13  RaceDesc                    311 non-null    object \n",
      " 14  DateofHire                  310 non-null    object \n",
      " 15  DateofTermination           104 non-null    object \n",
      " 16  TermReason                  311 non-null    object \n",
      " 17  EmploymentStatus            311 non-null    object \n",
      " 18  Department                  311 non-null    object \n",
      " 19  ManagerName                 311 non-null    object \n",
      " 20  ManagerID                   303 non-null    object \n",
      " 21  RecruitmentSource           311 non-null    object \n",
      " 22  PerformanceScore            311 non-null    object \n",
      " 23  EngagementSurvey            311 non-null    float64\n",
      " 24  EmpSatisfaction             311 non-null    int64  \n",
      " 25  SpecialProjectsCount        311 non-null    int64  \n",
      " 26  LastPerformanceReview_Date  311 non-null    object \n",
      " 27  DaysLateLast30              311 non-null    int64  \n",
      " 28  Absences                    311 non-null    int64  \n",
      " 29  Email                       311 non-null    object \n",
      " 30  Phone                       311 non-null    object \n",
      " 31  Salary_Fixed                299 non-null    object \n",
      "dtypes: float64(1), int64(7), object(24)\n",
      "memory usage: 77.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Zip_Fixed'] = df.loc[df['Zip'].str.isnumeric(), ['Zip']]\n",
    "\n",
    "df_invalid_zip = df.loc[df['Zip_Fixed'].isna(), ['Zip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1960\n",
       "1      2148\n",
       "2      1810\n",
       "3      1886\n",
       "4      2169\n",
       "       ... \n",
       "306    1810\n",
       "307    2458\n",
       "308    2067\n",
       "309    2148\n",
       "310    1730\n",
       "Name: Zip_Fixed, Length: 311, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Clean the column ``ManagerID`` by removing invalid values.\n",
    "\n",
    "    Invalid values are defined as any value that is not an integer or is an integer below 1.\n",
    "\n",
    "    Perform the selection of valid values and store them in column ``ManagerID_Fixed`` while invalid values should be ``NaN``. Then select invalid values and store the results in the variable ``df_invalid_managerID``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ManagerID'] = pd.to_numeric(df['ManagerID'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ManagerID_Fixed'] = pd.to_numeric(df['ManagerID'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalid_managerID = df.loc[df['ManagerID_Fixed'].isna(),['ManagerID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ManagerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ManagerID\n",
       "0        22.0\n",
       "1         4.0\n",
       "2        20.0\n",
       "3        16.0\n",
       "4          No\n",
       "..        ...\n",
       "306      20.0\n",
       "307      12.0\n",
       "308       2.0\n",
       "309       4.0\n",
       "310      14.0\n",
       "\n",
       "[310 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_invalid_managerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
